{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caedff9",
   "metadata": {},
   "source": [
    "yOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50fc1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO11n model\n",
    "yolo_model = YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c63d1c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\djadh\\Downloads\\Analyze Gpt\\images\\ss.jpg: 640x544 1 person, 119.9ms\n",
      "Speed: 6.5ms preprocess, 119.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n"
     ]
    }
   ],
   "source": [
    "results = yolo_model(\"images/ss.jpg\")  # Predict on an image\n",
    "results[0].show()  # Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2d819",
   "metadata": {},
   "source": [
    "YOLO E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d317748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\djadh\\Downloads\\Analyze Gpt\\images\\ss.jpg: 640x544 5 painos, 294.5ms\n",
      "Speed: 2.9ms preprocess, 294.5ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 544)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLOE\n",
    "\n",
    "# Initialize a YOLOE model\n",
    "model = YOLOE(\"yoloe-v8s-seg.pt\")  # or select yoloe-11s/m-seg.pt for different sizes\n",
    "\n",
    "# Set text prompt\n",
    "names = [\"paino\"]\n",
    "model.set_classes(names, model.get_text_pe(names))\n",
    "\n",
    "# Execute prediction for specified categories on an image\n",
    "results = model.predict(\"images/ss.jpg\")\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf23a890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'paino'}\n",
       " obb: None\n",
       " orig_img: array([[[219, 219, 219],\n",
       "         [219, 219, 219],\n",
       "         [219, 219, 219],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[219, 219, 219],\n",
       "         [219, 219, 219],\n",
       "         [219, 219, 219],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[219, 219, 219],\n",
       "         [219, 219, 219],\n",
       "         [219, 219, 219],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 40,  40,  40],\n",
       "         [ 41,  41,  41],\n",
       "         [ 32,  32,  32],\n",
       "         ...,\n",
       "         [115, 115, 115],\n",
       "         [110, 110, 110],\n",
       "         [113, 113, 113]],\n",
       " \n",
       "        [[ 82,  82,  82],\n",
       "         [ 88,  88,  88],\n",
       "         [ 60,  60,  60],\n",
       "         ...,\n",
       "         [103, 103, 103],\n",
       "         [ 96,  96,  96],\n",
       "         [116, 116, 116]],\n",
       " \n",
       "        [[ 81,  81,  81],\n",
       "         [ 81,  81,  81],\n",
       "         [ 37,  37,  37],\n",
       "         ...,\n",
       "         [ 76,  76,  76],\n",
       "         [ 70,  70,  70],\n",
       "         [100, 100, 100]]], shape=(765, 620, 3), dtype=uint8)\n",
       " orig_shape: (765, 620)\n",
       " path: 'c:\\\\Users\\\\djadh\\\\Downloads\\\\Analyze Gpt\\\\images\\\\ss.jpg'\n",
       " probs: None\n",
       " save_dir: 'c:\\\\Users\\\\djadh\\\\Downloads\\\\Analyze Gpt\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 2.905899891629815, 'inference': 294.5043998770416, 'postprocess': 8.33519990555942}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f04f7836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\djadh\\Downloads\\Analyze Gpt\\images\\ss.jpg: 640x544 (no detections), 280.1ms\n",
      "Speed: 3.0ms preprocess, 280.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n"
     ]
    }
   ],
   "source": [
    "model = YOLOE(\"yoloe-v8s-seg.pt\") \n",
    "results = model.predict(\"images/ss.jpg\")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116de3a2",
   "metadata": {},
   "source": [
    "Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9114358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e892ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045c7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd8e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960390c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d80176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201112d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae443d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2763099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c6bede",
   "metadata": {},
   "source": [
    "gemini model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f1603df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from config.constants import GEMINI_MODEL\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "gemini_model_client = OpenAIChatCompletionClient(\n",
    "model=\"gemini-1.5-flash-8b\",\n",
    "api_key=os.getenv('GOOGLE_API_KEY'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b0e64",
   "metadata": {},
   "source": [
    "TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ae43a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "def yolo_tool(image_path: str):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Runs YOLO object detection, segmentation, and pose estimation on the given image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing detected objects, confidence scores, and bounding boxes.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    results = yolo_model(image_path)\n",
    "    results[0].show()\n",
    "    classes = list({results[0].names[int(box.cls)] for box in results[0].boxes})\n",
    "    detections = [{\n",
    "        \"class_id\": int(box.cls),\n",
    "        \"class_name\": results[0].names[int(box.cls)],\n",
    "        \"confidence\": round(float(box.conf), 2)\n",
    "    } for box in results[0].boxes]\n",
    "    return {\"detected_classes\": classes, \"detailed_detections\": detections}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b9b42",
   "metadata": {},
   "source": [
    "AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bb7f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "\n",
    "yolo_agent = AssistantAgent(\n",
    "name = \"Object_Detection_agent\",\n",
    "model_client=gemini_model_client,\n",
    "tools = [yolo_tool],\n",
    "description=\"An agent that is an expert in Object detection , segmentation , pose estimation\",\n",
    "system_message=\"\"\"\n",
    "You are an AI agent specialized in computer vision tasks, with advanced capabilities in:  \n",
    "- Object detection  \n",
    "- Instance segmentation  \n",
    "- Pose estimation  \n",
    "\n",
    "You will be provided with an image as input with path\n",
    "you should give that path to your yolo_tool  \n",
    "Your task is to use the YOLO Tool to:  \n",
    "1. Detect all objects present in the image.  \n",
    "2. Perform segmentation for each detected object.  \n",
    "3. Conduct pose estimation where applicable.  \n",
    "\n",
    "After running the YOLO Tool:  \n",
    "- Summarize the results clearly in natural language, highlighting key \n",
    "findings such as object types, counts, and any notable poses or segmentation details.  \n",
    "- Conclude your response by saying **\"TERMINATE\"**.  \n",
    "\n",
    "**Important Rule:**  \n",
    "Only use the YOLO Tool when the query is directly related to object detection, segmentation, or pose estimation.  \n",
    "If the query is unrelated, provide the most relevant response instead, and still end with **\"TERMINATE\"**.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8662f",
   "metadata": {},
   "source": [
    "TEAMS with termination conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d9bc181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "team = RoundRobinGroupChat(\n",
    "    participants=[yolo_agent],\n",
    "    max_turns=4,\n",
    "    termination_condition= TextMentionTermination(\"TERMINATE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f4552",
   "metadata": {},
   "source": [
    "Queries\n",
    "\n",
    "Multimodal Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb37ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- MultiModalMessage (user) ----------\n",
      "Can you describe something about the images and also\n",
      "             the exact number of people in this image at path 'images/oo.jpeg'\n",
      "<image>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- ToolCallRequestEvent (Object_Detection_agent) ----------\n",
      "[FunctionCall(id='5', arguments='{\"image_path\": \"images/oo.jpeg\"}', name='yolo_tool')]\n",
      "image 1/1 c:\\Users\\djadh\\Downloads\\Analyze Gpt\\images\\oo.jpeg: 512x640 14 persons, 1 tie, 87.6ms\n",
      "Speed: 2.0ms preprocess, 87.6ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "---------- ToolCallExecutionEvent (Object_Detection_agent) ----------\n",
      "[FunctionExecutionResult(content=\"{'detected_classes': ['person', 'tie'], 'detailed_detections': [{'class_id': 0, 'class_name': 'person', 'confidence': 0.86}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.81}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.8}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.8}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.72}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.66}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.64}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.63}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.62}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.61}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.58}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.45}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.43}, {'class_id': 27, 'class_name': 'tie', 'confidence': 0.41}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.32}]}\", name='yolo_tool', call_id='5', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (Object_Detection_agent) ----------\n",
      "{'detected_classes': ['person', 'tie'], 'detailed_detections': [{'class_id': 0, 'class_name': 'person', 'confidence': 0.86}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.81}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.8}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.8}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.72}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.66}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.64}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.63}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.62}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.61}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.58}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.45}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.43}, {'class_id': 27, 'class_name': 'tie', 'confidence': 0.41}, {'class_id': 0, 'class_name': 'person', 'confidence': 0.32}]}\n",
      "---------- TextMessage (Object_Detection_agent) ----------\n",
      "The image at path 'images/oo.jpeg' contains multiple people, as well as one person wearing a tie. The exact number of people in the image is 13. These individuals have varying degrees of confidence in their detection, ranging from 0.32 to 0.86.\n",
      "\n",
      "It's worth noting that the image likely contains additional details beyond just the detected people and ties, such as clothing, accessories, or background information.\n",
      "\n",
      "\n",
      "TERMINATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The image at path 'images/oo.jpeg' contains multiple people, as well as one person wearing a tie. The exact number of people in the image is 13. These individuals have varying degrees of confidence in their detection, ranging from 0.32 to 0.86.\\n\\nIt's worth noting that the image likely contains additional details beyond just the detected people and ties, such as clothing, accessories, or background information.\\n\\n\\nTERMINATE\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.messages import MultiModalMessage\n",
    "from autogen_core import Image\n",
    "from PIL import Image as PILImage\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Load image from file\n",
    "pil_image = PILImage.open(\"images/oo.jpeg\")\n",
    "\n",
    "# Create AutoGen-compatible Image object\n",
    "img = Image(pil_image)\n",
    "\n",
    "# Create a multi-modal message\n",
    "multi_modal_message = MultiModalMessage(\n",
    "    content=[\"\"\"Can you describe something about the images and also\n",
    "             the exact number of people in this image at path 'images/oo.jpeg'\"\"\", img],\n",
    "    source=\"user\" \n",
    ")\n",
    "\n",
    "res = await Console(team.run_stream(task = multi_modal_message))\n",
    "res.messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e5ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
